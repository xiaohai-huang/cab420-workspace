{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from stemming.porter2 import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_doc(file_path, stop_words):\n",
    "    word_count = 0\n",
    "    doc_id = \"\"\n",
    "    term_frequency = {}\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text_start = False\n",
    "        for line in file:\n",
    "            # remove \\n and spaces\n",
    "            line = line.strip()\n",
    "\n",
    "            # process the content of <text></text>\n",
    "           \n",
    "\n",
    "            # obtain itemid\n",
    "            if not doc_id:\n",
    "                if line.startswith(\"<newsitem \"):\n",
    "                    for part in line.split():\n",
    "                        if part.startswith(\"itemid=\"):\n",
    "                            doc_id = part.split(\"=\")[1].strip(\"\\\"\")\n",
    "\n",
    "            # look for the content of <text></text>\n",
    "            if line.startswith(\"<text>\"):\n",
    "                text_start = True\n",
    "                continue\n",
    "            elif line.startswith(\"</text>\"):\n",
    "                text_start = False\n",
    "                break\n",
    "\n",
    "            if text_start:\n",
    "                # remove p tags\n",
    "                line = line.replace(\"<p>\",\"\").replace(\"</p>\", \"\")\n",
    "                \n",
    "                # remove digits and punctuations\n",
    "                line = line.translate(str.maketrans(\"\", \"\", string.digits))\n",
    "                line = line.translate(str.maketrans(string.punctuation, \" \"* len(string.punctuation)))\n",
    "\n",
    "                # remove extra white spaces\n",
    "                line = re.sub(r\"\\s+\", \" \", line)\n",
    "                for term in line.split():\n",
    "                    word_count += 1\n",
    "                    term = term.lower()\n",
    "                    term = stem(term)\n",
    "                    if len(term) > 2 and (term not in stop_words):\n",
    "                        if term in term_frequency:\n",
    "                            term_frequency[term] += 1\n",
    "                        else:\n",
    "                            term_frequency[term] = 1\n",
    "\n",
    "    return (word_count, {doc_id: term_frequency})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document itemid: 6146 contains: 133 words and 72 terms\n"
     ]
    }
   ],
   "source": [
    "stopwords_f = open('common-english-words.txt', 'r') # wk3\n",
    "stop_words = stopwords_f.read().split(',')\n",
    "x = parse_doc(\"6146.xml\",stop_words)\n",
    "\n",
    "for doc in x[1].items():\n",
    "        print('Document itemid: '+ doc[0]+ ' contains: '+ str(x[0]) + ' words and ' + str(len(doc[1])) + ' terms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Define `Doc_Node` and `List_Docs` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc_Node:\n",
    "    def __init__(self, data, next=None):\n",
    "        # (docid, curr_doc)\n",
    "        self.data = data\n",
    "        self.next = next\n",
    "\n",
    "class List_Docs:\n",
    "    def __init__(self, head_node:Doc_Node):\n",
    "        self.head = head_node\n",
    "        self.tail = head_node\n",
    "    \n",
    "    def insert(self, new_node:Doc_Node):\n",
    "        \"\"\"append new_node to the end of the linked list\"\"\"\n",
    "        self.tail.next = new_node\n",
    "        self.tail = new_node\n",
    "    \n",
    "    def lprint(self):\n",
    "        # (ID-6146: 72 terms) --> (ID-741299: 96 terms)\n",
    "        output = \"\"\n",
    "        current = self.head\n",
    "        while current:\n",
    "            doc_id, term_frequency = current.data\n",
    "            output += f\"(ID-{doc_id}: {len(term_frequency)} terms)\"\n",
    "            if current.next:\n",
    "                output += \" --> \"\n",
    "            current = current.next\n",
    "        print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_doc(file_path, stop_words):\n",
    "    word_count = 0\n",
    "    doc_id = \"\"\n",
    "    term_frequency = {}\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text_start = False\n",
    "        for line in file:\n",
    "            # remove \\n and spaces\n",
    "            line = line.strip()\n",
    "\n",
    "            # process the content of <text></text>\n",
    "           \n",
    "\n",
    "            # obtain itemid\n",
    "            if not doc_id:\n",
    "                if line.startswith(\"<newsitem \"):\n",
    "                    for part in line.split():\n",
    "                        if part.startswith(\"itemid=\"):\n",
    "                            doc_id = part.split(\"=\")[1].strip(\"\\\"\")\n",
    "\n",
    "            # look for the content of <text></text>\n",
    "            if line.startswith(\"<text>\"):\n",
    "                text_start = True\n",
    "                continue\n",
    "            elif line.startswith(\"</text>\"):\n",
    "                text_start = False\n",
    "                break\n",
    "\n",
    "            if text_start:\n",
    "                # remove p tags\n",
    "                line = line.replace(\"<p>\",\"\").replace(\"</p>\", \"\")\n",
    "                \n",
    "                # remove digits and punctuations\n",
    "                line = line.translate(str.maketrans(\"\", \"\", string.digits))\n",
    "                line = line.translate(str.maketrans(string.punctuation, \" \"* len(string.punctuation)))\n",
    "\n",
    "                # remove extra white spaces\n",
    "                line = re.sub(r\"\\s+\", \" \", line)\n",
    "                for term in line.split():\n",
    "                    word_count += 1\n",
    "                    term = term.lower()\n",
    "                    term = stem(term)\n",
    "                    if len(term) > 2 and (term not in stop_words):\n",
    "                        if term in term_frequency:\n",
    "                            term_frequency[term] += 1\n",
    "                        else:\n",
    "                            term_frequency[term] = 1\n",
    "\n",
    "    return (doc_id, term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ID-6146: 72 terms) --> (ID-741299: 96 terms)\n"
     ]
    }
   ],
   "source": [
    "files = [\"6146.xml\", \"../../week3/prac/741299newsML(1).xml\"]\n",
    "data = parse_doc(files[0], stop_words)\n",
    "node = Doc_Node(data)\n",
    "doc_list = List_Docs(node)\n",
    "\n",
    "\n",
    "for i in range(1, len(files)):\n",
    "    data = parse_doc(files[i], stop_words)\n",
    "    node = Doc_Node(data)\n",
    "    doc_list.insert(node)\n",
    "\n",
    "doc_list.lprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12328a4b23be1c7cec08b06706c715be0163e3764006085465cbbe8cea75b2e4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('cab420-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
